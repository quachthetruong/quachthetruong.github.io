[{"content":"Part 1: Introduction and Code Bucket Sort is an efficient sorting algorithm when input values are uniformly distributed over a range. It works by distributing elements into different \u0026ldquo;buckets\u0026rdquo;, sorting each bucket, and then concatenating the results.\nHere’s a typical Python implementation where each bucket is sorted with Insertion Sort:\ndef insertion_sort(bucket): for i in range(1, len(bucket)): key = bucket[i] j = i - 1 while j \u0026gt;= 0 and bucket[j] \u0026gt; key: bucket[j + 1] = bucket[j] j -= 1 bucket[j + 1] = key def bucket_sort(arr): n = len(arr) buckets = [[] for _ in range(n)] # Put array elements in different buckets for num in arr: bi = int(n * num) # assuming input numbers are in [0,1) buckets[bi].append(num) # Sort individual buckets using insertion sort for bucket in buckets: insertion_sort(bucket) # Concatenate all buckets into arr[] index = 0 for bucket in buckets: for num in bucket: arr[index] = num index += 1 Why Insertion Sort? Insertion sort is simple and efficient for small or nearly sorted lists. Since each bucket contains only a fraction of the input, sorting each bucket with insertion sort is fast.\nInsertion sort complexity: Sorting a bucket with n elements costs O(n²).\nPart 2: Mathematical Proof Setting Up the Problem:\nn: total number of elements. k: number of buckets. n_i: number of elements in bucket i. We model the assignment of elements to buckets using indicator random variables:\nUsing this, we express the size of each bucket n_i as:\nThis setup allows us to compute the expected value of n_i^2 which is crucial for bounding the sorting time in each bucket.\nWe want calculate expected value of n_i^2:\nSplit the sum:\nIn the final expansion, the summation splits into two cases: when j=l and when j≠l. Since each element is equally likely to go into any bucket, the probability that a given element j ends up in bucket i is 1/k. This means the indicator variable X_ij equals 1 with probability 1/k, and 0 otherwise.\nSo, we compute the expectations:\nAnd when j≠l, because element assignments are independent:\nSubstituting this into the total cost:\nFinal Complexity\nTherefore, if k=n, i.e. we use n buckets, the expected time complexity becomes:\nPart 3: Why Isn’t Bucket Sort Popular in Practice? On paper, Bucket Sort sounds amazing — it\u0026rsquo;s one of the few sorting algorithms that can achieve linear time. But in reality, you’ll rarely see it used in production systems or standard libraries like Python’s sort() or Java’s Arrays.sort().\nWhy? It comes down to strict limitations:\nIt only works well on uniformly distributed data. If your input values are clustered or uneven, bucket sort can slow down dramatically.\nIt needs a lot of memory. In the best case, you create as many buckets as input elements, which is often impractical.\nIt’s not in-place. That means it copies data around, consuming extra space compared to in-place algorithms like quicksort.\nIt doesn’t generalize easily. For example, it can’t handle complex comparison logic, custom comparators, or non-numeric types well.\nSo, despite its theoretical speed, Bucket Sort is rarely the right tool for general-purpose sorting. But in specific, controlled use cases, the bucket idea can still be useful:\nDistributed systems like Apache Spark or Hadoop use a similar idea called bucket partitioning — breaking data into ranges to parallelize processing.\nRadix Sort, a cousin of bucket sort, is used in systems where data can be broken down into digits or bytes — like sorting IP addresses, phone numbers, or fixed-length IDs — and works extremely well.\nPart 4: Conclusion In this article, we\u0026rsquo;ve broken down why Bucket Sort can theoretically run in O(n) time, and how that result depends on strong assumptions like uniform distribution and using many buckets.\nBut in practice, these ideal conditions are rare. That’s likely why Bucket Sort doesn’t show up much in popular tools or libraries.\nIf you’ve seen Bucket Sort used in a real application or system — not just as an example in a textbook — I’d love to hear about it. I\u0026rsquo;m still looking for real-world use cases beyond the theory.\n","permalink":"https://quachthetruong.github.io/posts/technical/bucket-sort-time-complexity/","summary":"\u003ch3 id=\"part-1-introduction-and-code\"\u003ePart 1: Introduction and Code\u003c/h3\u003e\n\u003cp\u003eBucket Sort is an efficient sorting algorithm when input values are uniformly distributed over a range. It works by distributing elements into different \u0026ldquo;buckets\u0026rdquo;, sorting each bucket, and then concatenating the results.\u003c/p\u003e\n\u003cp\u003eHere’s a typical Python implementation where each bucket is sorted with \u003ccode\u003eInsertion Sort\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003einsertion_sort\u003c/span\u003e(bucket):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, len(bucket)):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        key \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e bucket[i]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        j \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e bucket[j] \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e key:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            bucket[j \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e bucket[j]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            j \u003cspan style=\"color:#f92672\"\u003e-=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        bucket[j \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e key\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ebucket_sort\u003c/span\u003e(arr):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e len(arr)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    buckets \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e _ \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(n)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Put array elements in different buckets\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e num \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e arr:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        bi \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(n \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e num)  \u003cspan style=\"color:#75715e\"\u003e# assuming input numbers are in [0,1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        buckets[bi]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(num)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Sort individual buckets using insertion sort\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e bucket \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e buckets:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        insertion_sort(bucket)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Concatenate all buckets into arr[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e bucket \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e buckets:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e num \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e bucket:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            arr[index] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e num\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            index \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"why-insertion-sort\"\u003eWhy Insertion Sort?\u003c/h4\u003e\n\u003cp\u003eInsertion sort is simple and efficient for small or nearly sorted lists. Since each bucket contains only a fraction of the input, sorting each bucket with insertion sort is fast.\u003c/p\u003e","title":"Why the Average Complexity of Bucket sort is O(n)?"},{"content":"Differentiation is a key concept in machine learning, especially when optimizing functions like loss functions in neural networks. It helps us find the minimum of these functions, which is crucial for tasks like training a model. But have you ever wondered how popular libraries like TensorFlow and PyTorch perform differentiation? Let’s break it down!\n1. Manual Differentiation: The Old-School Method In school, we learn how to manually compute derivatives using calculus. You apply a set of rules to functions to find how they change with respect to their inputs. For example, given a simple function like:\nWe could compute the partial derivatives with respect to x and y as follows:\nThis method works well for simple functions, but as functions become more complex, the process of differentiation becomes tedious and prone to errors. It’s not scalable for large models, especially those used in machine learning.\n2. Finite Difference Approximation: A Simpler, But Less Accurate Method Finite difference approximation is a numerical method for calculating derivatives without explicit formulas. It approximates the derivative as the slope of a secant line between two points on the function. The derivative at point x0 is defined as:\nWhere ε is a small value (e.g., 10^(-5))\nExample with Python Code # f(x, y) = x^2 y + y + 2 with (x,y)=(3,4) def f(x, y): return x**2 * y + y + 2 def derivative(f, x, y, x_eps, y_eps): return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps) df_dx = derivative(f, 3, 4, 0.00001, 0) df_dy = derivative(f, 3, 4, 0, 0.00001) print(f\u0026#34;df_dx: {df_dx}\u0026#34;) # 24.000039999805264 print(f\u0026#34;df_dy: {df_dy}\u0026#34;) # 10.000000000331966 Limitations While simple, finite difference approximation is imprecise and becomes inefficient with many parameters. If there were 1000 parameters, we would need to call f() at least 1001 times. When you are dealing with large neural networks, this makes finite difference approximation way too inefficient.\n3. Forward-Mode Autodiff The figure below demonstrates forward-mode autodiff applied to the simple function 𝑔(𝑥,𝑦)=5+𝑥𝑦. The left graph shows the function, while the right graph represents the partial derivative ∂𝑔/∂x=0+(0⋅𝑥+𝑦⋅1)=𝑦. A similar process can be used to obtain the partial derivative with respect to 𝑦\nThe forward-mode autodiff algorithm works by traversing the computation graph from inputs to outputs. It starts by computing the partial derivatives of the leaf nodes. For instance, the constant node 5 returns 0 (since the derivative of a constant is 0), 𝑥 returns 1 (since ∂𝑥/∂𝑥=1), and 𝑦 returns 0 (since ∂𝑦/∂𝑥=0).\nNext, we move to the multiplication node, where the product rule is applied: ∂(𝑢⋅𝑣)/∂𝑥=∂𝑣/∂𝑥⋅𝑢+𝑣⋅∂𝑢/∂𝑥. This gives the expression 0⋅𝑥+𝑦⋅1.\nFinally, at the addition node, the derivative of a sum is the sum of the derivatives, so we combine the parts to get ∂𝑔/∂𝑥=0+(0⋅𝑥+𝑦⋅1).\nThough the equation can be simplified further to ∂𝑔/∂𝑥=𝑦, but imagine if our function had a variable 𝑧— we would need to calculate the entire graph once more. In real-world scenarios, the function may have many more variables, making the differentiation process much more complex and time-consuming.\n4. Reverse-Mode Autodiff Reverse-mode autodiff is a powerful technique commonly used in machine learning. It involves two passes through the computation graph. The first pass computes the value of each node from inputs to output. The second pass works in reverse (from output to inputs) to compute all partial derivatives. This process is known as \u0026ldquo;reverse mode\u0026rdquo; because gradients flow backward through the graph.\nThe graph bellow illustrates the second pass. During the first pass, all node values are computed starting from 𝑥 = 3 and 𝑦 = 4, with results shown at the bottom of each node (e.g., 𝑥 × 𝑥 = 9). The output node, 𝑓(3,4), results in 42.\nThe reverse pass applies the chain rule to compute partial derivatives. The chain rule is given by:\nwhere each n𝑖 represents an intermediate node in the graph.\nExample of Calculating Partial Derivatives Let’s go through the reverse pass step by step:\nAt n7 (the output node): Since n7 is the output node, f=n7, and the derivative with respect to n7 is 1.\nAt n5: Since n7 = n6 + n5, we have:\nThus, the partial derivative is: At n4: Since n5 = n4 * n2, we have:\nThus, the partial derivative is:\nThis process continues for all the nodes in the graph. In the end, we will have computed all partial derivatives of 𝑓(𝑥,𝑦) at 𝑥 = 3 and 𝑦 = 4. In this example, we find:\nKey Benefits of Reverse-Mode Autodiff Reverse-mode autodiff is particularly efficient when there are many inputs and fewer outputs, as it only requires one forward pass and one reverse pass per output. This makes it ideal for machine learning tasks, especially when training models like neural networks where there is typically only one output (e.g., the loss). This means only two passes through the graph are needed to compute all the gradients with respect to the model parameters.\nReverse-mode autodiff can also handle functions that are not entirely differentiable, as long as the partial derivatives can be computed at differentiable points.\nConclusion Having explored how computers perform differentiation, it’s clear why reverse-mode autodiff is the dominant solution in machine learning. The key advantage of reverse-mode is that it only requires two passes—one forward and one reverse—per output, regardless of the number of inputs. This makes it highly efficient, especially for large models where there are many parameters but typically only a single output, like the loss function in neural networks. Reverse-mode autodiff allows for fast, scalable optimization, making it the primary approach in modern machine learning frameworks.\nReferences https://github.com/ageron/handson-ml3 https://www.youtube.com/watch?v=wG_nF1awSSY https://srijithr.gitlab.io/post/autodiff/ https://www.tensorflow.org/guide/autodiff ","permalink":"https://quachthetruong.github.io/posts/technical/auto-differentiation/","summary":"\u003cp\u003eDifferentiation is a key concept in machine learning, especially when optimizing functions like loss functions in neural networks. It helps us find the minimum of these functions, which is crucial for tasks like training a model. But have you ever wondered how popular libraries like \u003cstrong\u003eTensorFlow\u003c/strong\u003e and \u003cstrong\u003ePyTorch\u003c/strong\u003e perform differentiation? Let’s break it down!\u003c/p\u003e\n\u003ch2 id=\"1-manual-differentiation-the-old-school-method\"\u003e1. Manual Differentiation: The Old-School Method\u003c/h2\u003e\n\u003cp\u003eIn school, we learn how to manually compute derivatives using calculus. You apply a set of rules to functions to find how they change with respect to their inputs. For example, given a simple function like:\u003c/p\u003e","title":"How Computers Do Differentiation?"},{"content":"For most developers, QuickSort is a fast and efficient sorting algorithm with a time complexity of O(nlogn). This makes it significantly better than other common sorting algorithms, like Selection Sort or Bubble Sort, which have a time complexity of O(n²). However, the question remains: Why is the average time complexity of QuickSort O(nlogn)?\nIn this blog, we will delve deep into the mathematical and probabilistic principles that explain this efficiency, helping you understand the underlying reasons why QuickSort is faster than other algorithms on average.\nQuickSort Basics: A Reminder Before diving into the mathematical reasoning, let’s quickly remind ourselves how QuickSort works. QuickSort is a divide-and-conquer sorting algorithm that recursively partitions an array into two subsets: one with elements smaller than a pivot value and the other with elements larger than the pivot value. This partitioning continues until the array is fully sorted.\nHere\u0026rsquo;s the code that implements the partitioning process in QuickSort:\n# Partition function def partition(arr, low, high): pivot = arr[high] i = low - 1 for j in range(low, high): if arr[j] \u0026lt; pivot: i += 1 swap(arr, i, j) swap(arr, i + 1, high) return i + 1 # Swap function def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i] # The QuickSort function implementation def quickSort(arr, low, high): if low \u0026lt; high: pi = partition(arr, low, high) quickSort(arr, low, pi - 1) quickSort(arr, pi + 1, high) How Can an Algorithm Be Considered Effective? An algorithm is considered effective if it solves a problem efficiently, especially with the least number of comparisons.\nFor example, consider the following JavaScript code that sorts an array of words based on their length:\nconst words = [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;kiwi\u0026#34;, \u0026#34;pear\u0026#34;]; words.sort((a, b) =\u0026gt; a.length - b.length); In this case, the number of comparisons refers to how many times the expression a.length - b.length is evaluated. The function compares the length of each word and orders them accordingly. The question we need to answer is: How many comparisons does this algorithm make?\nAn effective sorting algorithm minimizes unnecessary comparisons. For instance, if we’ve already compared two elements (e.g., \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;kiwi\u0026rdquo;), there’s no need to compare them again unless it\u0026rsquo;s necessary. This reduces the total number of comparisons, thus improving the algorithm’s efficiency.\nOptimal sorting algorithms handle this efficiently by avoiding redundant checks. For example, if we know that \u0026ldquo;kiwi\u0026rdquo; is already longer than \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;banana\u0026rdquo; is shorter, there’s no need to directly compare \u0026ldquo;kiwi\u0026rdquo; with \u0026ldquo;banana\u0026rdquo; again.\nWhat is Average Time Complexity? To understand average time complexity, we need to review some fundamental concepts from probability and statistics that every Vietnamese student learns in their first year of university. If you\u0026rsquo;re unfamiliar with it, you can take a few minutes to revisit this concept on Expected Value.\nLet’s break down the concept with an example:\nWe start with an ordered array:\noriginal = [1, 3, 4, 5, 6, 8, 9]; After shuffling the elements:\nshuffled = [3, 6, 5, 1, 4, 8, 9]; Now, we define a random variable 𝑋𝑖𝑗 that equals 1 if the algorithm does compare the i-th smallest and j-th smallest elements in the original array, and 0 if it does not. Let 𝑋 denote the total number of comparisons made by the algorithm. Since the algorithm never compares the same pair of elements twice, we have:\nTherefore, the expected value of 𝑋, denoted E[𝑋], is:\nUnderstanding E[𝑋𝑖𝑗] Now, let’s consider one of these 𝑋𝑖𝑗’s for i \u0026lt; j. Denote the i-th smallest element in the array by e𝑖and the j-th smallest element by e𝑗. Conceptually, imagine lining up the elements in sorted order. There are three possible cases for the pivot selection during QuickSort:\nCase 1: The pivot is between e𝑖 and 𝑒𝑗 In this case, the two elements e𝑖 and 𝑒𝑗end up in different partitions, and we will never compare them. This is because the pivot has separated these two elements into separate subsets.\nCase 2: The pivot is exactly e𝑖 or 𝑒𝑗 If the pivot chosen during the partitioning step is either e𝑖 or 𝑒𝑗, then we will compare these two elements directly because they are now in the same subset.\nCase 3: The pivot is less than e𝑖 or greater than 𝑒𝑗 You might wonder: What happens if the pivot is less than e𝑖 or greater than 𝑒𝑗? In these situations, the pivot does not directly affect the comparison between e𝑖 and 𝑒𝑗. Once the partitioning step occurs, both e𝑖 and 𝑒𝑗 will still end up in the same subset. Ultimately, they will converge into one of the two scenarios above where they are compared directly, and thus this case does not contribute to the expectation.\nAt each step, the probability that 𝑋𝑖𝑗 = 1 (i.e., we compare e𝑖 and 𝑒𝑗) is exactly 2/(j−i+1). Therefore, the overall probability that 𝑋𝑖𝑗 = 1 is:\nSumming Up the Expected Value This means that for a given element i, it is compared to element i+1 with probability 1, to element i+2 with probability 2/3, to element i+3 with probability 2/4, and so on. Therefore, the expected value of X is:\nThe sum of the series 1 + 1/2 + 1/3 + ... + 1/n, denoted 𝐻𝑛, is called the nth harmonic number. This series grows logarithmically and can be approximated as:\nln is the natural logarithm and γ is the Euler-Mascheroni constant, approximately 0.577. Since γ is a constant, it does not affect the overall growth rate of the sum. Therefore, in Big-O notation, we can express the growth of 𝐻𝑛 as O(lnn), meaning that as n increases,the harmonic number grows logarithmically.\nThus, we can bound the expected value of 𝑋 as:\nConclusion Through this blog, we’ve seen how QuickSort’s average complexity of O(nlogn) arises from the expected value, driven by the harmonic series. While QuickSort is often used as an example, many sorting algorithms can also be understood probabilistically, just like this. Whether or not you see this as essential knowledge, it’s an interesting and indispensable topic in understanding algorithm efficiency at a deeper level. The formulas and concepts we used may seem abstract, but they’re a powerful tool for analyzing and optimizing algorithms.\nReferences https://www.cs.cmu.edu/afs/cs/academic/class/15451-s07/www/lecture_notes/lect0123.pdf https://en.wikipedia.org/wiki/Harmonic_series_(mathematics) https://en.wikipedia.org/wiki/Expected_value https://www.geeksforgeeks.org/quick-sort-algorithm/ ","permalink":"https://quachthetruong.github.io/posts/technical/quick-sort-time-complexity/","summary":"\u003cp\u003eFor most developers, QuickSort is a fast and efficient sorting algorithm with a time complexity of \u003ccode\u003eO(nlogn)\u003c/code\u003e. This makes it significantly better than other common sorting algorithms, like Selection Sort or Bubble Sort, which have a time complexity of \u003ccode\u003eO(n²)\u003c/code\u003e. However, the question remains: \u003cstrong\u003eWhy is the average time complexity of QuickSort O(nlogn)?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn this blog, we will delve deep into the mathematical and probabilistic principles that explain this efficiency, helping you understand the underlying reasons why QuickSort is faster than other algorithms on average.\u003c/p\u003e","title":"Why the Average Complexity of QuickSort is O(nlogn)?"},{"content":"Part 1: Motivation How do we check if something is in a set — fast? The simplest way is a List:\nif x in items: ... But this is O(n) — too slow for large-scale systems.\nA HashSet improves to O(1) lookups on average, but it stores the full elements, requiring more memory than raw data — especially for strings or objects.\nSo what if we trade a little accuracy for massive savings? What if a structure could:\nUse only ~9.6 bits per element (significantly smaller than storing object or string),\nBe wrong only 1% of the time (false positives),\nAnd never say \u0026ldquo;no\u0026rdquo; to something that’s truly there?\nThat’s the power of the Bloom Filter.\nWhere is it used? Bloom filters are quietly at the heart of many systems:\nLSM trees, the foundation of modern NoSQL databases like Apache Cassandra, MongoDB, use Bloom filters to skip disk reads — asking: “Does this file probably contain the key?”\nPlatforms like Quora, Medium, and Yahoo use Bloom filters to:\nPrevent duplicate content,\nAvoid redundant processing,\nSpeed up internal caching systems.\nEven if you don’t see them — Bloom filters are working behind the scenes, making large systems fast and efficient.\nPart 2: What is a false positive? (Definition + Example) What is a false positive? ✅ When you check an element that was inserted, the Bloom filter says “yes” — that’s a true positive, and it’s 100% guaranteed correct.\n⚠️ But sometimes it says “yes” to something that was never inserted — that’s a false positive.\nA false positive happens when a new element matches the bit pattern of others — even though it was never added.\nThe filter sees all bits set and says “Probably yes” — but it’s wrong.\nThat’s the trade-off for speed and space — and you’ll see it clearly in the next example.\nHow does it work? A Bloom filter is built with:\nm: a bit array of length m, all bits start at 0\nk: k independent hash functions\nn: number of elements inserted\nTo insert an element O(1):\nHash it using all k functions\nFlip the corresponding k bits to 1\nTo check membership O(1):\nHash the element using the same k functions\nIf any of the k bits is 0 → the element is definitely not in the set\nIf all are 1 → the element is possibly in the set (could be a false positive)\nExample: When a False Positive Happens Let’s step through a small Bloom filter:\nSetup: m = 10 (bit array of size 10)\nk = 3 hash functions\nInserted elements: \u0026lsquo;apple\u0026rsquo; and \u0026lsquo;banana\u0026rsquo; so n = 2\nStart with an empty bit array:\nInitial: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Step 1: Insert \u0026lsquo;apple\u0026rsquo; Let’s say:\nh₁(apple) = 2\nh₂(apple) = 4\nh₃(apple) = 7\nUpdate the bit array:\nAfter insert apple: [0, 0, 1, 0, 1, 0, 0, 1, 0, 0] Step 2: Insert \u0026lsquo;banana\u0026rsquo; Let’s say:\nh₁(banana) = 1\nh₂(banana) = 4\nh₃(banana) = 8\nUpdate the bit array:\nAfter insert banana: [0, 1, 1, 0, 1, 0, 0, 1, 1, 0] Step 3: Check \u0026lsquo;banana\u0026rsquo; (True Positive) h₁(banana) = 1 → bit is 1\nh₂(banana) = 4 → bit is 1\nh₃(banana) = 8 → bit is 1\nAll bits are 1 → Bloom filter says “Yes” → ✅ correct!\nStep 4: Check \u0026lsquo;mango\u0026rsquo; (False Positive) h₁(mango) = 2\nh₂(mango) = 4\nh₃(mango) = 7\nCheck bits:\n2 → 1 ✅\n4 → 1 ✅\n7 → 1 ✅\nBloom filter says “Yes”, but \u0026lsquo;mango\u0026rsquo; was never inserted → ❌ false positive\nThis happens because \u0026lsquo;apple\u0026rsquo; and \u0026lsquo;banana\u0026rsquo; already set those bits.\nPart 3: Mathematical Proof Okay — now you understand what a false positive is. Let’s take it a step deeper and explore the probability theory behind Bloom filters.\nDon’t worry — it only uses basic math that every student learns in university. We\u0026rsquo;ll walk through this step-by-step, keeping things intuitive.\nWhat is our goal? We want to answer:\nWhat’s the probability that a new element (not in the set) returns a false positive?\nThat means: all the k bits checked during the query are already 1— even though the element was never inserted, not even once among the n inserted items.\nStep 1: Probability a Bit is Still 0 After One Flip Suppose we have an array of m bits, all starting as 0. Now we flip 1 random bit to 1. The chance that a specific bit stays 0 is:\nWhy? Because we only had a 1/m chance to hit it.\nStep 2: We Perform k × n Bit Flips We insert n elements, each hashed with k functions. So we flip bits k × n times.\nThe probability that a specific bit is still 0 after all those flips is:\nStep 3: Exponential Limit Approximation When m is large and kn is not too huge, we can approximate this with the exponential function:\nThis comes from the identity:\nStep 4: Probability Bit is 1 (i.e. Flipped at Least Once) This tells us how likely a bit is to be 1 after inserting n elements.\nStep 5: False Positive = All k Bits Are 1 Now, suppose we query a new element that wasn’t inserted. Its k hash functions give us k bit positions. The probability that all those k bits are already 1 — just by chance — is:\nAnd that’s the final formula for Bloom filter false positives.\nBloom Filter Example: 1 Million Items (n = 1,000,000) Size per element (m/n) Total Size Hash Functions (k) False Positive Rate 6.25 bits 0.78 MB 4 4.99% 7.5 bits 0.94 MB 5 2.70% 9.58 bits 1.20 MB 6 1.00% 12.5 bits 1.56 MB 8 0.25% Part 4: What’s Next — Other Smart Probabilistic Structures for Big Data Problems Bloom filters solve set membership efficiently — but what about other fundamental questions in computer science?\nHow many unique users have viewed this video? → HyperLogLog\nHow many times did user X access this page? → Count-Min Sketch\nWhat are the 50th, 90th, and 99th percentiles of the measured latencies? → t-digest\nWant more? → Explore more probabilistic data structures.\nReferences https://www.amazon.com/Probabilistic-Data-Structures-Algorithms-Applications/dp/3748190484 https://en.wikipedia.org/wiki/Category:Probabilistic_data_structures https://brilliant.org/wiki/bloom-filter/ https://redis.io/docs/latest/develop/data-types/probabilistic/ https://en.wikipedia.org/wiki/Bloom_filter ","permalink":"https://quachthetruong.github.io/posts/technical/bloom-filters-explained/","summary":"\u003ch2 id=\"part-1-motivation\"\u003ePart 1: Motivation\u003c/h2\u003e\n\u003ch3 id=\"how-do-we-check-if-something-is-in-a-set--fast\"\u003eHow do we check if something is in a set — fast?\u003c/h3\u003e\n\u003cp\u003eThe simplest way is a \u003ccode\u003eList\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e x \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e items:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBut this is \u003ccode\u003eO(n)\u003c/code\u003e — too slow for large-scale systems.\u003c/p\u003e\n\u003cp\u003eA \u003ccode\u003eHashSet\u003c/code\u003e improves to \u003ccode\u003eO(1)\u003c/code\u003e lookups on average,\nbut it stores the full elements, requiring \u003cstrong\u003emore memory than raw data\u003c/strong\u003e — especially for strings or objects.\u003c/p\u003e\n\u003ch3 id=\"so-what-if-we-trade-a-little-accuracy-for-massive-savings\"\u003eSo what if we trade a little accuracy for massive savings?\u003c/h3\u003e\n\u003cp\u003eWhat if a structure could:\u003c/p\u003e","title":"Bloom Filters Explained: A Fast and Space-Efficient Probabilistic Solution"},{"content":"Hành trình 21 ngày một mình xuyên Việt, từ Hà Nội đến Thành phố Hồ Chí Minh dọc theo con đường ven biển.\nKhi đôi chân chưa mỏi và trái tim vẫn tràn đầy nhiệt huyết!❤️‍🔥❤️💕\n","permalink":"https://quachthetruong.github.io/posts/life/xuyen-viet/","summary":"\u003cp\u003eHành trình 21 ngày một mình xuyên Việt, từ Hà Nội đến Thành phố Hồ Chí Minh dọc theo con đường ven biển.\u003c/p\u003e\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HjRp0UJY0YI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\u003cp\u003eKhi đôi chân chưa mỏi và trái tim vẫn tràn đầy nhiệt huyết!❤️‍🔥❤️💕\u003c/p\u003e","title":"Xuyên Việt 2025"}]