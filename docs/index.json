[{"content":"When diving deep into the EXPLAIN command, you already know what index scans, sequential scans, and hash joins are. But have you ever wondered how exactly the cost numbers are calculated? In this deep dive, we\u0026rsquo;ll explore the formulas behind PostgreSQL\u0026rsquo;s cost estimation for the three main scanning approaches: sequential scan, index scan, and bitmap heap scan.\n1. Cost-based vs Rule-based Consider this query:\nSELECT * FROM table_a ta JOIN table_b tb ON ta.id = tb.foreign_id WHERE ta.status = \u0026#39;active\u0026#39; AND tb.created_at \u0026gt; \u0026#39;2020-12-01\u0026#39;; How would you approach this query? The intuitive flow would be: filter each table first, then join. Make tables smaller first, then start joining. Quite intuitive, right?\nThis intuitive approach represents rule-based optimization - applying fixed rules regardless of data characteristics. But what if table_b has only 100 rows while table_a has 10 million? The join could eliminate 99.99% of the data before the expensive filter operation. In the early days of databases, query optimizers used rule-based approaches that couldn\u0026rsquo;t handle such scenarios effectively.\nThis is where cost-based optimization shines - the solution PostgreSQL adopted. PostgreSQL imagines all possible execution scenarios - considering table statistics, cardinality, storage characteristics (SSD vs HDD), and different access methods (sequential scan, index scan, nested loop join, merge join). It then calculates costs for each approach and picks the cheapest one.\nHere\u0026rsquo;s how PostgreSQL shows these costs in practice:\nEXPLAIN SELECT * FROM users; QUERY PLAN -------------------------------------------------------- Seq Scan on users (cost=0.00..450.00 rows=10000 width=64) This shows four key numbers:\nstartup_cost: 0.00 (sequential scan starts immediately) total_cost: 450.00 (cost to scan entire table) rows: 10000 (estimated number of rows) width: 64 (average row size in bytes) Here are the key constants PostgreSQL uses in cost estimation:\n# Cost model components: startup_cost # Time before first tuple (includes finding in index file, sorting) run_cost # Time to fetch data and perform operations (filtering, joins, sorting, etc.) total_cost # startup_cost + run_cost (total execution cost) # PostgreSQL cost constants (from src/backend/optimizer/path/costsize.c): seq_page_cost # Cost of a sequential page fetch (default: 1.0) random_page_cost # Cost of a non-sequential page fetch (default: 4.0) cpu_tuple_cost # Cost of typical CPU time to process a tuple (default: 0.01) cpu_index_tuple_cost # Cost of typical CPU time to process an index tuple (default: 0.005) cpu_operator_cost # Cost of CPU time to execute an operator or function (default: 0.0025) # Index cost multipliers (from src/backend/utils/adt/selfuncs.c): DEFAULT_PAGE_CPU_MULTIPLIER # Multiplier for CPU cost per B-tree page descended (default: 50x cpu_operator_cost) # Query variables: N_tuple # Total number of tuples in table N_page # Total number of pages in table N_index_tuple # Total number of tuples in index N_index_page # Total number of pages in index Selectivity # Proportion of search range satisfying WHERE clause (0 to 1) H_index # Height of internal tree levels (not including leaf level) These are dimensionless units for relative comparison, not absolute performance.\n2. Sequential Scan For sequential scans, the startup cost is always 0 since there\u0026rsquo;s no index traversal needed - PostgreSQL can read the first tuple immediately from the data file. Let\u0026rsquo;s estimate the cost for this query:\nSELECT * FROM tbl WHERE id \u0026lt;= 8000; The run cost is calculated using:\nrun_cost = cpu_cost + io_cost run_cost = (cpu_tuple_cost + cpu_operator_cost) * N_tuple + seq_page_cost * N_page Retrieve table statistics:\nSELECT relpages, reltuples FROM pg_class WHERE relname = \u0026#39;tbl\u0026#39;; relpages | reltuples ----------+----------- 45 | 10000 (1 row) N_tuple = 10000 N_page = 45 Therefore:\nrun_cost = (0.01 + 0.0025) * 10000 + 1.0 * 45 # = 170 Finally:\ntotal_cost = startup_cost + run_cost # = 0 + 170 = 170 For confirmation, the result of the EXPLAIN command of the above query is shown below:\nEXPLAIN SELECT * FROM tbl WHERE id \u0026lt;= 8000; QUERY PLAN -------------------------------------------------------- Seq Scan on tbl (cost=0.00..170.00 rows=8000 width=8) Filter: (id \u0026lt;= 8000) (2 rows) In the output above, we can see that the startup and total costs are 0.00 and 170.00, respectively, which matches our calculation perfectly. PostgreSQL estimates that 8000 rows will be selected by scanning all rows.\nEven though the condition id \u0026lt;= 8000 matches only 8000 rows, sequential scan still reads the entire table (10000 rows) and applies the filter during tuple processing.\nImportant: As understood from the run-cost estimation, PostgreSQL assumes that all pages will be read from storage. In other words, PostgreSQL does not consider whether the scanned page is in the shared buffers or not.\n3. Index Scan Let\u0026rsquo;s explore how to estimate the index scan cost for the following query:\nSELECT id, data FROM tbl WHERE data \u0026lt;= 240; Before estimating the cost, the numbers of the index pages and index tuples are shown below:\nSELECT relpages, reltuples FROM pg_class WHERE relname = \u0026#39;tbl_data_idx\u0026#39;; relpages | reltuples ----------+----------- 30 | 10000 (1 row) N_index_tuple = 10000 N_index_page = 30 3.1. Startup Cost The startup cost represents the cost of reading index pages to access the first tuple. It has two components:\nstartup_cost = scan_cost + nav_cost startup_cost = ceil(log2(N_index_tuple)) * cpu_operator_cost + (H_index + 1) * DEFAULT_PAGE_CPU_MULTIPLIER * cpu_operator_cost Key comparisons: ceil(log2(N_index_tuple)) - CPU cost of binary search comparisons to locate key Page processing: (H_index + 1) * DEFAULT_PAGE_CPU_MULTIPLIER - CPU cost of processing pages while descending tree levels, including leaf level To find the actual tree height:\nCREATE EXTENSION pageinspect; SELECT btpo_level FROM bt_page_stats(\u0026#39;tbl_data_idx\u0026#39;, bt_metap(\u0026#39;tbl_data_idx\u0026#39;)-\u0026gt;root); btpo_level ------------ 1 (1 row) Therefore:\nstartup_cost = ceil(log2(10000)) * 0.0025 + (1 + 1) * 50 * 0.0025 # = 0.285 3.2. Run Cost The run cost of the index scan is the sum of the CPU costs and the I/O costs of both the table and the index:\nrun_cost = (index_cpu_cost + table_cpu_cost) + (index_io_cost + table_io_cost) Info: If Index-Only Scans can be applied, table_cpu_cost and table_io_cost are not estimated.\nThe first three costs (i.e., index CPU cost, table CPU cost, and index I/O cost) are shown below:\nindex_cpu_cost = Selectivity * N_index_tuple * (cpu_index_tuple_cost + cpu_operator_cost) table_cpu_cost = Selectivity * N_tuple * cpu_tuple_cost index_io_cost = ceil(Selectivity * N_index_page) * random_page_cost üìù Background: Selectivity Calculation\nThe selectivity of query predicates is estimated using either the histogram_bounds or the MCV (Most Common Value), both of which are stored in the statistics information in the pg_stats.\nMCV-based Selectivity The MCV of each column is stored in the pg_stats view as a pair of columns:\nSELECT most_common_vals, most_common_freqs FROM pg_stats WHERE tablename = \u0026#39;countries\u0026#39; AND attname=\u0026#39;continent\u0026#39;; ------------------+------------------------------------------------------------- most_common_vals | {\u0026#34;Africa\u0026#34;,\u0026#34;Europe\u0026#34;,\u0026#34;Asia\u0026#34;,\u0026#34;North America\u0026#34;,\u0026#34;Oceania\u0026#34;,\u0026#34;South America\u0026#34;} most_common_freqs | {0.274611,0.243523,0.227979,0.119171,0.0725389,0.0621762} (1 row) For a query like continent = 'Asia', the selectivity is the frequency corresponding to \u0026lsquo;Asia\u0026rsquo; in most_common_freqs, which is 0.227979.\nHistogram-based Selectivity If the MCV cannot be used, e.g., the target column type is integer or double precision, then the value of the histogram_bounds of the target column is used to estimate the cost.\nhistogram_bounds is a list of values that divide the column\u0026rsquo;s values into groups of approximately equal population.\nA specific example is shown. This is the value of the histogram_bounds of the column \u0026lsquo;data\u0026rsquo; in the table \u0026rsquo;tbl\u0026rsquo;:\nSELECT histogram_bounds FROM pg_stats WHERE tablename = \u0026#39;tbl\u0026#39; AND attname = \u0026#39;data\u0026#39;; histogram_bounds --------------------------------------------------------------------------------------------------- {1,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100, 2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100, 4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100, 6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100, 8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000} (1 row) By default, the histogram_bounds is divided into 100 buckets. Buckets are numbered starting from 0, and every bucket stores approximately the same number of tuples. The values of histogram_bounds are the bounds of the corresponding buckets:\nhistogram_bounds | hb(0) | hb(1) | hb(2) | hb(3) | ... | hb(99) | hb(100) -----------------+-------+-------+-------+-------+-----+--------+--------- | 1 | 100 | 200 | 300 | ... | 9900 | 10000 For our query data \u0026lt;= 240, the value 240 falls in bucket 2 (between hb[2] = 200 and hb[3] = 300). Using linear interpolation:\nposition_in_bucket = (target_value - bucket_min) / (bucket_max - bucket_min) Selectivity = (bucket_index + position_in_bucket) / total_buckets Selectivity = (2 + (240 - 200) / (300 - 200)) / 100 # = 0.024 Using our selectivity value of 0.024:\nindex_cpu_cost = 0.024 * 10000 * (0.005 + 0.0025) # = 1.8 table_cpu_cost = 0.024 * 10000 * 0.01 # = 2.4 index_io_cost = ceil(0.024 * 30) * 4.0 # = 4.0 The table_io_cost is defined by:\ntable_io_cost = max_io_cost - pow(index_correlation, 2) * (max_io_cost - min_io_cost) max_io_cost is the worst case I/O cost (randomly scanning all table pages): max_io_cost = N_page * random_page_cost max_io_cost = 45 * 4.0 # = 180 min_io_cost is the best case I/O cost (sequentially scanning selected pages): min_io_cost = 1 * random_page_cost + (ceil(Selectivity * N_page) - 1) * seq_page_cost min_io_cost = 1 * 4.0 + (ceil(0.024 * 45) - 1) * 1.0 # = 5 üìù Background: Index Correlation\nIndex correlation is a statistical correlation between the physical row ordering and the logical ordering of the column values. This ranges from -1 to +1.\nSELECT col,col_asc,col_desc,col_rand testdb-# FROM tbl_corr; col | col_asc | col_desc | col_rand ----------+---------+----------+---------- Tuple_1 | 1 | 12 | 3 Tuple_2 | 2 | 11 | 8 Tuple_3 | 3 | 10 | 5 Tuple_4 | 4 | 9 | 9 Tuple_5 | 5 | 8 | 7 Tuple_6 | 6 | 7 | 2 Tuple_7 | 7 | 6 | 10 Tuple_8 | 8 | 5 | 11 Tuple_9 | 9 | 4 | 4 Tuple_10 | 10 | 3 | 1 Tuple_11 | 11 | 2 | 12 Tuple_12 | 12 | 1 | 6 (12 rows) SELECT tablename,attname, correlation FROM pg_stats WHERE tablename = \u0026#39;tbl_corr\u0026#39;; tablename | attname | correlation -----------+----------+------------- tbl_corr | col_asc | 1 tbl_corr | col_desc | -1 tbl_corr | col_rand | 0.125874 (3 rows) Perfect correlation (1.0) means rows are stored in index order, enabling sequential access. Poor correlation (0.0) forces random page access.\nSummary: Higher correlation (closer to 1.0) ‚Üí Lower I/O costs due to sequential access. Lower correlation (closer to 0.0) ‚Üí Higher I/O costs due to random access.\nIn this example, index_correlation = 1.0:\ntable_io_cost = max_io_cost - pow(index_correlation, 2) * (max_io_cost - min_io_cost) table_io_cost = 180 - pow(1.0, 2) * (180 - 5) # = 5 Finally, we can calculate the total run cost:\nrun_cost = (index_cpu_cost + table_cpu_cost) + (index_io_cost + table_io_cost) run_cost = (1.8 + 2.4) + (4.0 + 5) # = 13.2 3.3. Total Cost According to the previous calculations:\ntotal_cost = startup_cost + run_cost total_cost = 0.285 + 13.2 # = 13.485 For confirmation, the result of the EXPLAIN command shows:\nEXPLAIN SELECT id, data FROM tbl WHERE data \u0026lt;= 240; QUERY PLAN ----------------------------------------------------------------------- Index Scan using tbl_data_idx on tbl (cost=0.29..13.49 rows=240 width=8) Index Cond: (data \u0026lt;= 240) (2 rows) Our calculated cost (13.485) closely matches PostgreSQL\u0026rsquo;s estimate (13.49), validating the formula accuracy.\nSSD Optimization: The default values assume random scans are four times slower than sequential scans, reflecting traditional HDD performance. For SSDs, consider reducing random_page_cost to around 1.0 for better query plans.\n4. Bitmap Scan Bitmap scans combine the benefits of index scans and sequential scans. Let\u0026rsquo;s set up a test table to examine bitmap scan cost estimation:\nCREATE TABLE foo (typ int, bar int, id1 int); CREATE INDEX ON foo(typ); CREATE INDEX ON foo(bar); CREATE INDEX ON foo(id1); INSERT INTO foo (typ, bar, id1) SELECT CAST(cos(2 * pi() * random()) * sqrt(-2 * ln(random())) * 100 AS integer), n % 97, n % 101 FROM generate_series(1, 1000000) n; VACUUM ANALYZE foo; Now let\u0026rsquo;s estimate the cost for this Bitmap Scan query:\nSELECT * FROM foo WHERE bar = 2; First, let\u0026rsquo;s get the table statistics:\nSELECT relpages, reltuples FROM pg_class WHERE relname = \u0026#39;foo\u0026#39;; relpages | reltuples ----------+----------- 5405 | 1000000 (1 row) N_tuple = 1000000 N_page = 5405 For the index statistics:\nSELECT relpages, reltuples FROM pg_class WHERE relname = \u0026#39;foo_bar_idx\u0026#39;; relpages | reltuples ----------+----------- 871 | 1000000 (1 row) N_index_page = 871 N_index_tuple = 1000000 The planner estimates the selectivity here at approximately:\nSelectivity = rows_returned / N_tuple Selectivity = 10200 / 1000000 # = 0.0102 The total Bitmap Index Scan cost is calculated identically to the plain Index Scan cost, except for table scans:\n# Calculate selected pages and tuples from index pages = N_index_page * Selectivity pages = 871 * 0.0102 # = 8.8842 tuples = round(N_index_tuple * Selectivity) tuples = round(1000000 * 0.0102) # = 10200 # Bitmap Index Scan cost bitmap_index_cost = round(random_page_cost * pages + cpu_index_tuple_cost * tuples + cpu_operator_cost * tuples) bitmap_index_cost = round(4.0 * 8.8842 + 0.005 * 10200 + 0.0025 * 10200) # = 112 The Bitmap Heap Scan I/O cost calculation differs from Index Scan. Pages are fetched in ascending order without repeats, but tuples are no longer sequentially located, increasing the number of pages to fetch.\nüìù Background: Mackert-Lohman Formula\nThe Mackert-Lohman formula estimates the number of pages fetched in a bitmap scan, based on the coupon collector problem. It calculates how many unique pages contain the selected tuples:\n# First calculate tuples fetched tuples_fetched = N_tuple * Selectivity tuples_fetched = 1000000 * 0.0102 # = 10200 # Apply Mackert-Lohman formula pages_fetched = min((2 * N_page * tuples_fetched) / (2 * N_page + tuples_fetched), N_page) pages_fetched = min((2 * 5405 * 10200) / (2 * 5405 + 10200), 5405) # = 5248.54 The formula accounts for the probability of tuple distribution across pages, with the min() ensuring we never fetch more pages than the table actually has.\nThe fetch cost of a single page is estimated somewhere between seq_page_cost and random_page_cost, depending on the fraction of the total number of pages to be fetched.\n# Calculate cost per page (interpolated between seq and random) cost_per_page = random_page_cost - (random_page_cost - seq_page_cost) * (pages_fetched / N_page) ** 0.5 cost_per_page = 4.0 - (4.0 - 1.0) * (5248.54 / 5405) ** 0.5 # = 1.0440 The Bitmap Heap Scan includes processing costs for scanned tuples and recheck operations. PostgreSQL adds bitmap manipulation overhead of 1.1 * cpu_operator_cost per expected index tuple: 0.1 for building the bitmap (startup cost) and 1.0 for rechecking tuples (run cost).\n# Bitmap Heap Scan startup cost (includes bitmap building cost) bitmap_ops_multiplier = 0.1 # Bitmap building cost: 0.1 * cpu_operator_cost per index tuple startup_cost = round(bitmap_index_cost + bitmap_ops_multiplier * cpu_operator_cost * N_tuple * Selectivity) startup_cost = round(112 + 0.1 * 0.0025 * 1000000 * 0.0102) # = 115 # Bitmap Heap Scan run cost run_cost = round(cost_per_page * pages_fetched + cpu_tuple_cost * tuples_fetched + cpu_operator_cost * tuples_fetched) run_cost = round(1.0440 * 5248.54 + 0.01 * 10200 + 0.0025 * 10200) # = 5607 # Total cost total_cost = startup_cost + run_cost total_cost = 115 + 5607 # = 5722 For confirmation, the result of the EXPLAIN command shows:\nEXPLAIN SELECT * FROM foo WHERE bar = 2; QUERY PLAN -------------------------------------------------------------------------------- Bitmap Heap Scan on foo (cost=115.47..5722.32 rows=10200 width=12) Recheck Cond: (bar = 2) -\u0026gt; Bitmap Index Scan on foo_bar_idx (cost=0.00..112.92 rows=10200 width=0) Index Cond: (bar = 2) (4 rows) Our calculated costs are very close to PostgreSQL\u0026rsquo;s estimates: startup cost (115 vs 115.47) and total cost (5722 vs 5722.32). The small differences are due to rounding in our step-by-step calculations, additional internal optimizations in PostgreSQL\u0026rsquo;s implementation, and floating-point precision differences. This close match validates that our cost estimation formulas capture the core logic correctly.\nConclusion I initially wanted to define simple selectivity thresholds: 5% use index scan, 5-10% use bitmap scan, above 15% use sequential scan. However, after diving deep into PostgreSQL\u0026rsquo;s cost calculations, I realize: just let the optimizer do its job. The complexity of factors like index correlation, page distribution, and hardware characteristics makes manual rules impractical. Our job is simply to create valuable indexes and trust PostgreSQL\u0026rsquo;s cost-based optimizer.\nReferences https://www.interdb.jp/pg/pgsql03/02.html https://www.postgresql.org/docs/current/using-explain.html https://www.rockdata.net/tutorial/plan-bitmap-scan/ https://gist.github.com/SamsadSajid/40e14bbb9157f53f44cca08d1e9eba39 https://github.com/postgres/postgres/blob/master/src/backend/optimizer/path/costsize.c ","permalink":"https://quachthetruong.github.io/posts/technical/postgres-cost-estimation/","summary":"\u003cp\u003eWhen diving deep into the \u003ccode\u003eEXPLAIN\u003c/code\u003e command, you already know what index scans, sequential scans, and hash joins are. But have you ever wondered how exactly the cost numbers are calculated? In this deep dive, we\u0026rsquo;ll explore the formulas behind PostgreSQL\u0026rsquo;s cost estimation for the three main scanning approaches: sequential scan, index scan, and bitmap heap scan.\u003c/p\u003e\n\u003ch2 id=\"postgresql-cost-constants\"\u003e1. Cost-based vs Rule-based\u003c/h2\u003e\n\u003cp\u003eConsider this query:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eSELECT\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eFROM\u003c/span\u003e table_a ta\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eJOIN\u003c/span\u003e table_b tb \u003cspan style=\"color:#66d9ef\"\u003eON\u003c/span\u003e ta.id \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tb.foreign_id\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eWHERE\u003c/span\u003e ta.status \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;active\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eAND\u003c/span\u003e tb.created_at \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;2020-12-01\u0026#39;\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHow would you approach this query? The intuitive flow would be: filter each table first, then join. Make tables smaller first, then start joining. Quite intuitive, right?\u003c/p\u003e","title":"How PostgreSQL Evaluates Execution Plans: Cost Estimation Deep Dive"},{"content":"Part 1: Introduction and Code Bucket Sort is an efficient sorting algorithm when input values are uniformly distributed over a range. It works by distributing elements into different \u0026ldquo;buckets\u0026rdquo;, sorting each bucket, and then concatenating the results.\nHere‚Äôs a typical Python implementation where each bucket is sorted with Insertion Sort:\ndef insertion_sort(bucket): for i in range(1, len(bucket)): key = bucket[i] j = i - 1 while j \u0026gt;= 0 and bucket[j] \u0026gt; key: bucket[j + 1] = bucket[j] j -= 1 bucket[j + 1] = key def bucket_sort(arr): n = len(arr) buckets = [[] for _ in range(n)] # Put array elements in different buckets for num in arr: bi = int(n * num) # assuming input numbers are in [0,1) buckets[bi].append(num) # Sort individual buckets using insertion sort for bucket in buckets: insertion_sort(bucket) # Concatenate all buckets into arr[] index = 0 for bucket in buckets: for num in bucket: arr[index] = num index += 1 Why Insertion Sort? Insertion sort is simple and efficient for small or nearly sorted lists. Since each bucket contains only a fraction of the input, sorting each bucket with insertion sort is fast.\nInsertion sort complexity: Sorting a bucket with n elements costs O(n¬≤).\nPart 2: Mathematical Proof Setting Up the Problem:\nn: total number of elements. k: number of buckets. n_i: number of elements in bucket i. We model the assignment of elements to buckets using indicator random variables:\nUsing this, we express the size of each bucket n_i as:\nThis setup allows us to compute the expected value of n_i^2 which is crucial for bounding the sorting time in each bucket.\nWe want calculate expected value of n_i^2:\nSplit the sum:\nIn the final expansion, the summation splits into two cases: when j=l and when j‚â†l. Since each element is equally likely to go into any bucket, the probability that a given element j ends up in bucket i is 1/k. This means the indicator variable X_ij equals 1 with probability 1/k, and 0 otherwise.\nSo, we compute the expectations:\nAnd when j‚â†l, because element assignments are independent:\nSubstituting this into the total cost:\nFinal Complexity\nTherefore, if k=n, i.e. we use n buckets, the expected time complexity becomes:\nPart 3: Why Isn‚Äôt Bucket Sort Popular in Practice? On paper, Bucket Sort sounds amazing ‚Äî it\u0026rsquo;s one of the few sorting algorithms that can achieve linear time. But in reality, you‚Äôll rarely see it used in production systems or standard libraries like Python‚Äôs sort() or Java‚Äôs Arrays.sort().\nWhy? It comes down to strict limitations:\nIt only works well on uniformly distributed data. If your input values are clustered or uneven, bucket sort can slow down dramatically.\nIt needs a lot of memory. In the best case, you create as many buckets as input elements, which is often impractical.\nIt‚Äôs not in-place. That means it copies data around, consuming extra space compared to in-place algorithms like quicksort.\nIt doesn‚Äôt generalize easily. For example, it can‚Äôt handle complex comparison logic, custom comparators, or non-numeric types well.\nSo, despite its theoretical speed, Bucket Sort is rarely the right tool for general-purpose sorting. But in specific, controlled use cases, the bucket idea can still be useful:\nDistributed systems like Apache Spark or Hadoop use a similar idea called bucket partitioning ‚Äî breaking data into ranges to parallelize processing.\nRadix Sort, a cousin of bucket sort, is used in systems where data can be broken down into digits or bytes ‚Äî like sorting IP addresses, phone numbers, or fixed-length IDs ‚Äî and works extremely well.\nPart 4: Conclusion In this article, we\u0026rsquo;ve broken down why Bucket Sort can theoretically run in O(n) time, and how that result depends on strong assumptions like uniform distribution and using many buckets.\nBut in practice, these ideal conditions are rare. That‚Äôs likely why Bucket Sort doesn‚Äôt show up much in popular tools or libraries.\nIf you‚Äôve seen Bucket Sort used in a real application or system ‚Äî not just as an example in a textbook ‚Äî I‚Äôd love to hear about it. I\u0026rsquo;m still looking for real-world use cases beyond the theory.\nReferences https://en.wikipedia.org/wiki/Bucket_sort https://www.geeksforgeeks.org/dsa/bucket-sort-2/ ","permalink":"https://quachthetruong.github.io/posts/technical/bucket-sort-time-complexity/","summary":"\u003ch3 id=\"part-1-introduction-and-code\"\u003ePart 1: Introduction and Code\u003c/h3\u003e\n\u003cp\u003eBucket Sort is an efficient sorting algorithm when input values are uniformly distributed over a range. It works by distributing elements into different \u0026ldquo;buckets\u0026rdquo;, sorting each bucket, and then concatenating the results.\u003c/p\u003e\n\u003cp\u003eHere‚Äôs a typical Python implementation where each bucket is sorted with \u003ccode\u003eInsertion Sort\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003einsertion_sort\u003c/span\u003e(bucket):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, len(bucket)):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        key \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e bucket[i]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        j \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e j \u003cspan style=\"color:#f92672\"\u003e\u0026gt;=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003eand\u003c/span\u003e bucket[j] \u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e key:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            bucket[j \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e bucket[j]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            j \u003cspan style=\"color:#f92672\"\u003e-=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        bucket[j \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e key\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ebucket_sort\u003c/span\u003e(arr):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    n \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e len(arr)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    buckets \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e [[] \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e _ \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(n)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Put array elements in different buckets\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e num \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e arr:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        bi \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e int(n \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e num)  \u003cspan style=\"color:#75715e\"\u003e# assuming input numbers are in [0,1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        buckets[bi]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(num)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Sort individual buckets using insertion sort\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e bucket \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e buckets:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        insertion_sort(bucket)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# Concatenate all buckets into arr[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    index \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e bucket \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e buckets:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e num \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e bucket:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            arr[index] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e num\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            index \u003cspan style=\"color:#f92672\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"why-insertion-sort\"\u003eWhy Insertion Sort?\u003c/h4\u003e\n\u003cp\u003eInsertion sort is simple and efficient for small or nearly sorted lists. Since each bucket contains only a fraction of the input, sorting each bucket with insertion sort is fast.\u003c/p\u003e","title":"Why the Average Complexity of Bucket sort is O(n)?"},{"content":"Differentiation is a key concept in machine learning, especially when optimizing functions like loss functions in neural networks. It helps us find the minimum of these functions, which is crucial for tasks like training a model. But have you ever wondered how popular libraries like TensorFlow and PyTorch perform differentiation? Let‚Äôs break it down!\n1. Manual Differentiation: The Old-School Method In school, we learn how to manually compute derivatives using calculus. You apply a set of rules to functions to find how they change with respect to their inputs. For example, given a simple function like:\nWe could compute the partial derivatives with respect to x and y as follows:\nThis method works well for simple functions, but as functions become more complex, the process of differentiation becomes tedious and prone to errors. It‚Äôs not scalable for large models, especially those used in machine learning.\n2. Finite Difference Approximation: A Simpler, But Less Accurate Method Finite difference approximation is a numerical method for calculating derivatives without explicit formulas. It approximates the derivative as the slope of a secant line between two points on the function. The derivative at point x0 is defined as:\nWhere Œµ is a small value (e.g., 10^(-5))\nExample with Python Code # f(x, y) = x^2 y + y + 2 with (x,y)=(3,4) def f(x, y): return x**2 * y + y + 2 def derivative(f, x, y, x_eps, y_eps): return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps) df_dx = derivative(f, 3, 4, 0.00001, 0) df_dy = derivative(f, 3, 4, 0, 0.00001) print(f\u0026#34;df_dx: {df_dx}\u0026#34;) # 24.000039999805264 print(f\u0026#34;df_dy: {df_dy}\u0026#34;) # 10.000000000331966 Limitations While simple, finite difference approximation is imprecise and becomes inefficient with many parameters. If there were 1000 parameters, we would need to call f() at least 1001 times. When you are dealing with large neural networks, this makes finite difference approximation way too inefficient.\n3. Forward-Mode Autodiff The figure below demonstrates forward-mode autodiff applied to the simple function ùëî(ùë•,ùë¶)=5+ùë•ùë¶. The left graph shows the function, while the right graph represents the partial derivative ‚àÇùëî/‚àÇx=0+(0‚ãÖùë•+ùë¶‚ãÖ1)=ùë¶. A similar process can be used to obtain the partial derivative with respect to ùë¶\nThe forward-mode autodiff algorithm works by traversing the computation graph from inputs to outputs. It starts by computing the partial derivatives of the leaf nodes. For instance, the constant node 5 returns 0 (since the derivative of a constant is 0), ùë• returns 1 (since ‚àÇùë•/‚àÇùë•=1), and ùë¶ returns 0 (since ‚àÇùë¶/‚àÇùë•=0).\nNext, we move to the multiplication node, where the product rule is applied: ‚àÇ(ùë¢‚ãÖùë£)/‚àÇùë•=‚àÇùë£/‚àÇùë•‚ãÖùë¢+ùë£‚ãÖ‚àÇùë¢/‚àÇùë•. This gives the expression 0‚ãÖùë•+ùë¶‚ãÖ1.\nFinally, at the addition node, the derivative of a sum is the sum of the derivatives, so we combine the parts to get ‚àÇùëî/‚àÇùë•=0+(0‚ãÖùë•+ùë¶‚ãÖ1).\nThough the equation can be simplified further to ‚àÇùëî/‚àÇùë•=ùë¶, but imagine if our function had a variable ùëß‚Äî we would need to calculate the entire graph once more. In real-world scenarios, the function may have many more variables, making the differentiation process much more complex and time-consuming.\n4. Reverse-Mode Autodiff Reverse-mode autodiff is a powerful technique commonly used in machine learning. It involves two passes through the computation graph. The first pass computes the value of each node from inputs to output. The second pass works in reverse (from output to inputs) to compute all partial derivatives. This process is known as \u0026ldquo;reverse mode\u0026rdquo; because gradients flow backward through the graph.\nThe graph bellow illustrates the second pass. During the first pass, all node values are computed starting from ùë• = 3 and ùë¶ = 4, with results shown at the bottom of each node (e.g., ùë• √ó ùë• = 9). The output node, ùëì(3,4), results in 42.\nThe reverse pass applies the chain rule to compute partial derivatives. The chain rule is given by:\nwhere each nùëñ represents an intermediate node in the graph.\nExample of Calculating Partial Derivatives Let‚Äôs go through the reverse pass step by step:\nAt n7 (the output node): Since n7 is the output node, f=n7, and the derivative with respect to n7 is 1.\nAt n5: Since n7 = n6 + n5, we have:\nThus, the partial derivative is: At n4: Since n5 = n4 * n2, we have:\nThus, the partial derivative is:\nThis process continues for all the nodes in the graph. In the end, we will have computed all partial derivatives of ùëì(ùë•,ùë¶) at ùë• = 3 and ùë¶ = 4. In this example, we find:\nKey Benefits of Reverse-Mode Autodiff Reverse-mode autodiff is particularly efficient when there are many inputs and fewer outputs, as it only requires one forward pass and one reverse pass per output. This makes it ideal for machine learning tasks, especially when training models like neural networks where there is typically only one output (e.g., the loss). This means only two passes through the graph are needed to compute all the gradients with respect to the model parameters.\nReverse-mode autodiff can also handle functions that are not entirely differentiable, as long as the partial derivatives can be computed at differentiable points.\nConclusion Having explored how computers perform differentiation, it‚Äôs clear why reverse-mode autodiff is the dominant solution in machine learning. The key advantage of reverse-mode is that it only requires two passes‚Äîone forward and one reverse‚Äîper output, regardless of the number of inputs. This makes it highly efficient, especially for large models where there are many parameters but typically only a single output, like the loss function in neural networks. Reverse-mode autodiff allows for fast, scalable optimization, making it the primary approach in modern machine learning frameworks.\nReferences https://github.com/ageron/handson-ml3 https://www.youtube.com/watch?v=wG_nF1awSSY https://srijithr.gitlab.io/post/autodiff/ https://www.tensorflow.org/guide/autodiff ","permalink":"https://quachthetruong.github.io/posts/technical/auto-differentiation/","summary":"\u003cp\u003eDifferentiation is a key concept in machine learning, especially when optimizing functions like loss functions in neural networks. It helps us find the minimum of these functions, which is crucial for tasks like training a model. But have you ever wondered how popular libraries like \u003cstrong\u003eTensorFlow\u003c/strong\u003e and \u003cstrong\u003ePyTorch\u003c/strong\u003e perform differentiation? Let‚Äôs break it down!\u003c/p\u003e\n\u003ch2 id=\"1-manual-differentiation-the-old-school-method\"\u003e1. Manual Differentiation: The Old-School Method\u003c/h2\u003e\n\u003cp\u003eIn school, we learn how to manually compute derivatives using calculus. You apply a set of rules to functions to find how they change with respect to their inputs. For example, given a simple function like:\u003c/p\u003e","title":"How Computers Do Differentiation?"},{"content":"For most developers, QuickSort is a fast and efficient sorting algorithm with a time complexity of O(nlogn). This makes it significantly better than other common sorting algorithms, like Selection Sort or Bubble Sort, which have a time complexity of O(n¬≤). However, the question remains: Why is the average time complexity of QuickSort O(nlogn)?\nIn this blog, we will delve deep into the mathematical and probabilistic principles that explain this efficiency, helping you understand the underlying reasons why QuickSort is faster than other algorithms on average.\nQuickSort Basics: A Reminder Before diving into the mathematical reasoning, let‚Äôs quickly remind ourselves how QuickSort works. QuickSort is a divide-and-conquer sorting algorithm that recursively partitions an array into two subsets: one with elements smaller than a pivot value and the other with elements larger than the pivot value. This partitioning continues until the array is fully sorted.\nHere\u0026rsquo;s the code that implements the partitioning process in QuickSort:\n# Partition function def partition(arr, low, high): pivot = arr[high] i = low - 1 for j in range(low, high): if arr[j] \u0026lt; pivot: i += 1 swap(arr, i, j) swap(arr, i + 1, high) return i + 1 # Swap function def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i] # The QuickSort function implementation def quickSort(arr, low, high): if low \u0026lt; high: pi = partition(arr, low, high) quickSort(arr, low, pi - 1) quickSort(arr, pi + 1, high) How Can an Algorithm Be Considered Effective? An algorithm is considered effective if it solves a problem efficiently, especially with the least number of comparisons.\nFor example, consider the following JavaScript code that sorts an array of words based on their length:\nconst words = [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;kiwi\u0026#34;, \u0026#34;pear\u0026#34;]; words.sort((a, b) =\u0026gt; a.length - b.length); In this case, the number of comparisons refers to how many times the expression a.length - b.length is evaluated. The function compares the length of each word and orders them accordingly. The question we need to answer is: How many comparisons does this algorithm make?\nAn effective sorting algorithm minimizes unnecessary comparisons. For instance, if we‚Äôve already compared two elements (e.g., \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;kiwi\u0026rdquo;), there‚Äôs no need to compare them again unless it\u0026rsquo;s necessary. This reduces the total number of comparisons, thus improving the algorithm‚Äôs efficiency.\nOptimal sorting algorithms handle this efficiently by avoiding redundant checks. For example, if we know that \u0026ldquo;kiwi\u0026rdquo; is already longer than \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;banana\u0026rdquo; is shorter, there‚Äôs no need to directly compare \u0026ldquo;kiwi\u0026rdquo; with \u0026ldquo;banana\u0026rdquo; again.\nWhat is Average Time Complexity? To understand average time complexity, we need to review some fundamental concepts from probability and statistics that every Vietnamese student learns in their first year of university. If you\u0026rsquo;re unfamiliar with it, you can take a few minutes to revisit this concept on Expected Value.\nLet‚Äôs break down the concept with an example:\nWe start with an ordered array:\noriginal = [1, 3, 4, 5, 6, 8, 9]; After shuffling the elements:\nshuffled = [3, 6, 5, 1, 4, 8, 9]; Now, we define a random variable ùëãùëñùëó that equals 1 if the algorithm does compare the i-th smallest and j-th smallest elements in the original array, and 0 if it does not. Let ùëã denote the total number of comparisons made by the algorithm. Since the algorithm never compares the same pair of elements twice, we have:\nTherefore, the expected value of ùëã, denoted E[ùëã], is:\nUnderstanding E[ùëãùëñùëó] Now, let‚Äôs consider one of these ùëãùëñùëó‚Äôs for i \u0026lt; j. Denote the i-th smallest element in the array by eùëñand the j-th smallest element by eùëó. Conceptually, imagine lining up the elements in sorted order. There are three possible cases for the pivot selection during QuickSort:\nCase 1: The pivot is between eùëñ and ùëíùëó In this case, the two elements eùëñ and ùëíùëóend up in different partitions, and we will never compare them. This is because the pivot has separated these two elements into separate subsets.\nCase 2: The pivot is exactly eùëñ or ùëíùëó If the pivot chosen during the partitioning step is either eùëñ or ùëíùëó, then we will compare these two elements directly because they are now in the same subset.\nCase 3: The pivot is less than eùëñ or greater than ùëíùëó You might wonder: What happens if the pivot is less than eùëñ or greater than ùëíùëó? In these situations, the pivot does not directly affect the comparison between eùëñ and ùëíùëó. Once the partitioning step occurs, both eùëñ and ùëíùëó will still end up in the same subset. Ultimately, they will converge into one of the two scenarios above where they are compared directly, and thus this case does not contribute to the expectation.\nAt each step, the probability that ùëãùëñùëó = 1 (i.e., we compare eùëñ and ùëíùëó) is exactly 2/(j‚àíi+1). Therefore, the overall probability that ùëãùëñùëó = 1 is:\nSumming Up the Expected Value This means that for a given element i, it is compared to element i+1 with probability 1, to element i+2 with probability 2/3, to element i+3 with probability 2/4, and so on. Therefore, the expected value of X is:\nThe sum of the series 1 + 1/2 + 1/3 + ... + 1/n, denoted ùêªùëõ, is called the nth harmonic number. This series grows logarithmically and can be approximated as:\nln is the natural logarithm and Œ≥ is the Euler-Mascheroni constant, approximately 0.577. Since Œ≥ is a constant, it does not affect the overall growth rate of the sum. Therefore, in Big-O notation, we can express the growth of ùêªùëõ as O(lnn), meaning that as n increases,the harmonic number grows logarithmically.\nThus, we can bound the expected value of ùëã as:\nConclusion Through this blog, we‚Äôve seen how QuickSort‚Äôs average complexity of O(nlogn) arises from the expected value, driven by the harmonic series. While QuickSort is often used as an example, many sorting algorithms can also be understood probabilistically, just like this. Whether or not you see this as essential knowledge, it‚Äôs an interesting and indispensable topic in understanding algorithm efficiency at a deeper level. The formulas and concepts we used may seem abstract, but they‚Äôre a powerful tool for analyzing and optimizing algorithms.\nReferences https://www.cs.cmu.edu/afs/cs/academic/class/15451-s07/www/lecture_notes/lect0123.pdf https://en.wikipedia.org/wiki/Harmonic_series_(mathematics) https://en.wikipedia.org/wiki/Expected_value https://www.geeksforgeeks.org/quick-sort-algorithm/ ","permalink":"https://quachthetruong.github.io/posts/technical/quick-sort-time-complexity/","summary":"\u003cp\u003eFor most developers, QuickSort is a fast and efficient sorting algorithm with a time complexity of \u003ccode\u003eO(nlogn)\u003c/code\u003e. This makes it significantly better than other common sorting algorithms, like Selection Sort or Bubble Sort, which have a time complexity of \u003ccode\u003eO(n¬≤)\u003c/code\u003e. However, the question remains: \u003cstrong\u003eWhy is the average time complexity of QuickSort O(nlogn)?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn this blog, we will delve deep into the mathematical and probabilistic principles that explain this efficiency, helping you understand the underlying reasons why QuickSort is faster than other algorithms on average.\u003c/p\u003e","title":"Why the Average Complexity of QuickSort is O(nlogn)?"},{"content":"Part 1: Motivation How do we check if something is in a set ‚Äî fast? The simplest way is a List:\nif x in items: ... But this is O(n) ‚Äî too slow for large-scale systems.\nA HashSet improves to O(1) lookups on average, but it stores the full elements, requiring more memory than raw data ‚Äî especially for strings or objects.\nSo what if we trade a little accuracy for massive savings? What if a structure could:\nUse only ~9.6 bits per element (significantly smaller than storing object or string),\nBe wrong only 1% of the time (false positives),\nAnd never say \u0026ldquo;no\u0026rdquo; to something that‚Äôs truly there?\nThat‚Äôs the power of the Bloom Filter.\nWhere is it used? Bloom filters are quietly at the heart of many systems:\nLSM trees, the foundation of modern NoSQL databases like Apache Cassandra, MongoDB, use Bloom filters to skip disk reads ‚Äî asking: ‚ÄúDoes this file probably contain the key?‚Äù\nPlatforms like Quora, Medium, and Yahoo use Bloom filters to:\nPrevent duplicate content,\nAvoid redundant processing,\nSpeed up internal caching systems.\nEven if you don‚Äôt see them ‚Äî Bloom filters are working behind the scenes, making large systems fast and efficient.\nPart 2: What is a false positive? (Definition + Example) What is a false positive? ‚úÖ When you check an element that was inserted, the Bloom filter says ‚Äúyes‚Äù ‚Äî that‚Äôs a true positive, and it‚Äôs 100% guaranteed correct.\n‚ö†Ô∏è But sometimes it says ‚Äúyes‚Äù to something that was never inserted ‚Äî that‚Äôs a false positive.\nA false positive happens when a new element matches the bit pattern of others ‚Äî even though it was never added.\nThe filter sees all bits set and says ‚ÄúProbably yes‚Äù ‚Äî but it‚Äôs wrong.\nThat‚Äôs the trade-off for speed and space ‚Äî and you‚Äôll see it clearly in the next example.\nHow does it work? A Bloom filter is built with:\nm: a bit array of length m, all bits start at 0\nk: k independent hash functions\nn: number of elements inserted\nTo insert an element O(1):\nHash it using all k functions\nFlip the corresponding k bits to 1\nTo check membership O(1):\nHash the element using the same k functions\nIf any of the k bits is 0 ‚Üí the element is definitely not in the set\nIf all are 1 ‚Üí the element is possibly in the set (could be a false positive)\nExample: When a False Positive Happens Let‚Äôs step through a small Bloom filter:\nSetup: m = 10 (bit array of size 10)\nk = 3 hash functions\nInserted elements: \u0026lsquo;apple\u0026rsquo; and \u0026lsquo;banana\u0026rsquo; so n = 2\nStart with an empty bit array:\nInitial: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Step 1: Insert \u0026lsquo;apple\u0026rsquo; Let‚Äôs say:\nh‚ÇÅ(apple) = 2\nh‚ÇÇ(apple) = 4\nh‚ÇÉ(apple) = 7\nUpdate the bit array:\nAfter insert apple: [0, 0, 1, 0, 1, 0, 0, 1, 0, 0] Step 2: Insert \u0026lsquo;banana\u0026rsquo; Let‚Äôs say:\nh‚ÇÅ(banana) = 1\nh‚ÇÇ(banana) = 4\nh‚ÇÉ(banana) = 8\nUpdate the bit array:\nAfter insert banana: [0, 1, 1, 0, 1, 0, 0, 1, 1, 0] Step 3: Check \u0026lsquo;banana\u0026rsquo; (True Positive) h‚ÇÅ(banana) = 1 ‚Üí bit is 1\nh‚ÇÇ(banana) = 4 ‚Üí bit is 1\nh‚ÇÉ(banana) = 8 ‚Üí bit is 1\nAll bits are 1 ‚Üí Bloom filter says ‚ÄúYes‚Äù ‚Üí ‚úÖ correct!\nStep 4: Check \u0026lsquo;mango\u0026rsquo; (False Positive) h‚ÇÅ(mango) = 2\nh‚ÇÇ(mango) = 4\nh‚ÇÉ(mango) = 7\nCheck bits:\n2 ‚Üí 1 ‚úÖ\n4 ‚Üí 1 ‚úÖ\n7 ‚Üí 1 ‚úÖ\nBloom filter says ‚ÄúYes‚Äù, but \u0026lsquo;mango\u0026rsquo; was never inserted ‚Üí ‚ùå false positive\nThis happens because \u0026lsquo;apple\u0026rsquo; and \u0026lsquo;banana\u0026rsquo; already set those bits.\nPart 3: Mathematical Proof Okay ‚Äî now you understand what a false positive is. Let‚Äôs take it a step deeper and explore the probability theory behind Bloom filters.\nDon‚Äôt worry ‚Äî it only uses basic math that every student learns in university. We\u0026rsquo;ll walk through this step-by-step, keeping things intuitive.\nWhat is our goal? We want to answer:\nWhat‚Äôs the probability that a new element (not in the set) returns a false positive?\nThat means: all the k bits checked during the query are already 1‚Äî even though the element was never inserted, not even once among the n inserted items.\nStep 1: Probability a Bit is Still 0 After One Flip Suppose we have an array of m bits, all starting as 0. Now we flip 1 random bit to 1. The chance that a specific bit stays 0 is:\nWhy? Because we only had a 1/m chance to hit it.\nStep 2: We Perform k √ó n Bit Flips We insert n elements, each hashed with k functions. So we flip bits k √ó n times.\nThe probability that a specific bit is still 0 after all those flips is:\nStep 3: Exponential Limit Approximation When m is large and kn is not too huge, we can approximate this with the exponential function:\nThis comes from the identity:\nStep 4: Probability Bit is 1 (i.e. Flipped at Least Once) This tells us how likely a bit is to be 1 after inserting n elements.\nStep 5: False Positive = All k Bits Are 1 Now, suppose we query a new element that wasn‚Äôt inserted. Its k hash functions give us k bit positions. The probability that all those k bits are already 1 ‚Äî just by chance ‚Äî is:\nAnd that‚Äôs the final formula for Bloom filter false positives.\nBloom Filter Example: 1 Million Items (n = 1,000,000) Size per element (m/n) Total Size Hash Functions (k) False Positive Rate 6.25 bits 0.78 MB 4 4.99% 7.5 bits 0.94 MB 5 2.70% 9.58 bits 1.20 MB 6 1.00% 12.5 bits 1.56 MB 8 0.25% Part 4: What‚Äôs Next ‚Äî Other Smart Probabilistic Structures for Big Data Problems Bloom filters solve set membership efficiently ‚Äî but what about other fundamental questions in computer science?\nHow many unique users have viewed this video? ‚Üí HyperLogLog\nHow many times did user X access this page? ‚Üí Count-Min Sketch\nWhat are the 50th, 90th, and 99th percentiles of the measured latencies? ‚Üí t-digest\nWant more? ‚Üí Explore more probabilistic data structures.\nReferences https://www.amazon.com/Probabilistic-Data-Structures-Algorithms-Applications/dp/3748190484 https://en.wikipedia.org/wiki/Category:Probabilistic_data_structures https://brilliant.org/wiki/bloom-filter/ https://redis.io/docs/latest/develop/data-types/probabilistic/ https://en.wikipedia.org/wiki/Bloom_filter ","permalink":"https://quachthetruong.github.io/posts/technical/bloom-filters-explained/","summary":"\u003ch2 id=\"part-1-motivation\"\u003ePart 1: Motivation\u003c/h2\u003e\n\u003ch3 id=\"how-do-we-check-if-something-is-in-a-set--fast\"\u003eHow do we check if something is in a set ‚Äî fast?\u003c/h3\u003e\n\u003cp\u003eThe simplest way is a \u003ccode\u003eList\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e x \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e items:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBut this is \u003ccode\u003eO(n)\u003c/code\u003e ‚Äî too slow for large-scale systems.\u003c/p\u003e\n\u003cp\u003eA \u003ccode\u003eHashSet\u003c/code\u003e improves to \u003ccode\u003eO(1)\u003c/code\u003e lookups on average,\nbut it stores the full elements, requiring \u003cstrong\u003emore memory than raw data\u003c/strong\u003e ‚Äî especially for strings or objects.\u003c/p\u003e\n\u003ch3 id=\"so-what-if-we-trade-a-little-accuracy-for-massive-savings\"\u003eSo what if we trade a little accuracy for massive savings?\u003c/h3\u003e\n\u003cp\u003eWhat if a structure could:\u003c/p\u003e","title":"Bloom Filters Explained: A Fast and Space-Efficient Probabilistic Solution"},{"content":"H√†nh tr√¨nh 21 ng√†y m·ªôt m√¨nh xuy√™n Vi·ªát, t·ª´ H√† N·ªôi ƒë·∫øn Th√†nh ph·ªë H·ªì Ch√≠ Minh d·ªçc theo con ƒë∆∞·ªùng ven bi·ªÉn.\nKhi ƒë√¥i ch√¢n ch∆∞a m·ªèi v√† tr√°i tim v·∫´n tr√†n ƒë·∫ßy nhi·ªát huy·∫øt!‚ù§Ô∏è‚Äçüî•‚ù§Ô∏èüíï\n","permalink":"https://quachthetruong.github.io/posts/life/xuyen-viet/","summary":"\u003cp\u003eH√†nh tr√¨nh 21 ng√†y m·ªôt m√¨nh xuy√™n Vi·ªát, t·ª´ H√† N·ªôi ƒë·∫øn Th√†nh ph·ªë H·ªì Ch√≠ Minh d·ªçc theo con ƒë∆∞·ªùng ven bi·ªÉn.\u003c/p\u003e\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HjRp0UJY0YI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\u003cp\u003eKhi ƒë√¥i ch√¢n ch∆∞a m·ªèi v√† tr√°i tim v·∫´n tr√†n ƒë·∫ßy nhi·ªát huy·∫øt!‚ù§Ô∏è‚Äçüî•‚ù§Ô∏èüíï\u003c/p\u003e","title":"Xuy√™n Vi·ªát 2025"}]